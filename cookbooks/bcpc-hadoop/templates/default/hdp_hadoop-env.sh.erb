# Set Hadoop-specific environment variables here.
 
# The java implementation to use.  Required.
export JAVA_HOME=<%= node["bcpc"]["hadoop"]["java"] %>
export HADOOP_HOME_WARN_SUPPRESS=1
 
# Extra Java runtime options.  Empty by default.
export HADOOP_OPTS="-Djava.net.preferIPv4Stack=true ${HADOOP_OPTS}"
 
# Where log files are stored.  $HADOOP_HOME/logs by default.
export HADOOP_LOG_DIR=/var/log/hadoop-hdfs
 
# History server logs
export HADOOP_MAPRED_LOG_DIR=/var/log/hadoop-mapreduce

# History server pid
export HADOOP_MAPRED_PID_DIR=/var/run/hadoop-mapreduce
 
# Where log files are stored in the secure data environment.
export HADOOP_SECURE_DN_LOG_DIR=/var/log/hadoop-hdfs

# The directory where pid files are stored.
export HADOOP_PID_DIR=/var/run/hadoop-hdfs
export HADOOP_SECURE_DN_PID_DIR=/var/run/hadoop-hdfs
 
# A string representing this instance of hadoop.
export HADOOP_IDENT_STRING="$USER"
 
# Use libraries from standard classpath
JAVA_JDBC_LIBS=""
#Add libraries required by mysql connector
for jarFile in `ls /usr/share/java/*mysql* 2>/dev/null`
do
  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile
done
#Add libraries required by oracle connector
for jarFile in `ls /usr/share/java/*ojdbc* 2>/dev/null`
do
  JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile
done
#Add libraries required by nodemanager
TEZ_LIBS=/etc/tez/conf:/usr/hdp/${HDP_VERSION}/tez/*:/usr/hdp/${HDP_VERSION}/tez/lib/*
export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}${JAVA_JDBC_LIBS}:${TEZ_LIBS}
 
# Path to jsvc required by secure datanode
export JSVC_HOME=/usr/lib/bigtop-utils
 
# On secure datanodes, user to run the datanode as after dropping privileges
export HADOOP_SECURE_DN_USER=hdfs

<%
   datanode_heap = [node["bcpc"]["hadoop"]["datanode"]["xmx"]["max_size"], (node["memory"]["total"].to_i * node["bcpc"]["hadoop"]["datanode"]["xmx"]["max_ratio"]/1024).floor].min
   datanode_newsize = [(0.125*datanode_heap).ceil, 3072].min

   namenode_heap = [node["bcpc"]["hadoop"]["namenode"]["xmx"]["max_size"], (node["memory"]["total"].to_i * node["bcpc"]["hadoop"]["namenode"]["xmx"]["max_ratio"]/1024).floor].min
   namenode_newsize = [(0.125*namenode_heap).ceil, 3072].min 

   maxpermsize = [(node["memory"]["total"].to_i * 0.021).floor, 256].min
-%>
export HADOOP_DATANODE_OPTS="$HADOOP_DATANODE_OPTS -Xms<%= datanode_heap %>m -Xmx<%= datanode_heap %>m -Xmn<%= datanode_newsize %>m -XX:PermSize=<%= (0.5*maxpermsize).floor %>m -XX:MaxPermSize=<%= maxpermsize %>m <%= node["bcpc"]["hadoop"]["datanode"]["gc_opts"] %>"
export HADOOP_NAMENODE_OPTS="$HADOOP_NAMENODE_OPTS -Xms<%= namenode_heap %>m -Xmx<%= namenode_heap %>m -Xmn<%= namenode_newsize %>m -XX:PermSize=<%= (0.5*maxpermsize).floor %>m -XX:MaxPermSize=<%= maxpermsize %>m <%= node["bcpc"]["hadoop"]["namenode"]["gc_opts"] %>"

<% if node[:bcpc][:hadoop].attribute?(:jmx_enabled) and node[:bcpc][:hadoop][:jmx_enabled] %>
JMX_OPTS="-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false"
export HADOOP_NAMENODE_OPTS="$HADOOP_NAMENODE_OPTS $JMX_OPTS -Dcom.sun.management.jmxremote.port=<%= @nn_jmx_port %>"
export HADOOP_DATANODE_OPTS="$HADOOP_DATANODE_OPTS $JMX_OPTS -Dcom.sun.management.jmxremote.port=<%= @dn_jmx_port %>"
<% end %>
